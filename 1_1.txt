When the tic-tac-toe learning algorithm played against an opponent, we might have assumed that the opponent had some 
non-randomly directed model of play toward a winning strategy. Setting the algorithm against this opponent may have 
led the value function more reliably and more quickly to converge upon a winning strategy itself. On the other hand, 
given that each game of tic-tac-toe ends after at most nine turns and always ends in one of three states -- victory, 
defeat, or tie -- it seems intuitive that two nascent models pitted against one other would find a means of improving 
their play, though perhaps after scrambling around randomly for a while, say, several thousand games or so. It might 
prove useful to evaluate the performance of these two models against known-good strategies, which we happen to have 
on hand since tic-tac-toe is a solved game.

Of course, some questions come to mind. How might this generative adversarial strategy fare in a more complex space, 
especially when starting from a position of no prior knowledge? Would they scramble around in the dark endlessly, or 
perhaps discover some obscure, unintuitive optimization route? How can we evaluate the performance of these models when 
expertise is harder to come by? Are we doomed to reinforce our models at the speed of human interaction, or can we give 
it some help by modeling aspects of expertise? If we can model expertise, why bother with this reinforcement learning 
stuff at all? These are, of course, very broad, low-resolution portrayals of the art that will hopefully be ameliorated 
in this course of study.